---
title: "S670 Project 3"
author: 
- The Fantastic Four
- Erik "Human Torch" Parker
- Emily "Invisible Woman" Rudman
- Vinay "The Thing" Vernekar
- Jervis "Mister Fantastic" Wang
date: "April 4, 2017"
output: pdf_document
---



```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(broom)
library(tidyr)
library(gridExtra)

load("CCES2016_Common_FirstRelease.RData")
election <- x
election <- election[election$tookpost == "Yes",]
election <- election[election$CC16_326 == "Barack Obama",]
election$switch <- ifelse(election$CC16_410a == "Donald Trump (Republican)", 1, 0)

election2 <- election %>% drop_na(switch)


election.model <- glm(switch ~ gender + educ + race, family = "quasibinomial", weights = commonweight_post, data = election2)

election.model.df <- election2
election.model.df$.fitted <- fitted.values(election.model)
election.model.df$.resid <- residuals(election.model, type = "response")

ggplot(election.model.df, aes(x = .fitted, y = .resid)) +geom_point() + geom_smooth(method = "loess", method.args = list(degree = 1))

switch.pred <- predict(election.model, type = "response", newdata = election2)

switch.pred.df <- data.frame(election.model.df, switch.prob = as.vector(switch.pred))

ggplot(switch.pred.df, aes(x = race, y = switch.prob, color = gender)) + facet_wrap("educ") + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = .9, vjust = .5))
```

> We tested a large number of models, and the one shown here had the best residual-fit plot, while also being one of the simplest.  Including interactions didn't do much to improve the model, and we decided to use a simple, fully additive model for ease of interpretation and to avoid overfitting.  In our sample there was a 0% probability of a Middle Eastern person switching from voting from Obama to Trump. In every category, males were about 5% more likely to switch their vote to Trump than females. Also, as education level went up, the probability of switching vote from Obama to Trump went down.


#2 Improve model
```{r, echo = FALSE}

election4 <- election2 %>% drop_na(sexuality)

election4$straight <- ifelse(election4$sexuality == "Heterosexual / straight", "yes", "no")

election.model.2 <- glm(switch ~ gender + educ + race + straight, family = "quasibinomial", weights = commonweight_post, data = election4)

election.model.df.2 <- election4
election.model.df.2$.fitted <- fitted.values(election.model.2)
election.model.df.2$.resid <- residuals(election.model.2, type = "response")


ggplot(election.model.df.2, aes(x = .fitted, y = .resid)) +geom_point() + geom_smooth(method = "loess", method.args = list(degree = 1))

switch.pred.2 <- predict(election.model.2, type = "response", newdata = election4)

switch.pred.df.2 <- data.frame(election.model.df.2, switch.prob = as.vector(switch.pred.2))

ggplot(switch.pred.df.2, aes(x = race, y = switch.prob, color = gender)) + facet_wrap(~ educ + straight) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = .9, vjust = .5))

```

> We decided to add sexuality for another factor. We made sexuality into a binomial factor so we could facet into heterosexual or not.   When we included it in our model without any interactions, the residual fit plot looked slightly better than our original model. In our faceted plots we noticed that heterosexual people in almost every category were more likely to switch to Trump than people who are not.  Additionally, we also found that the residual deviance decreased by around 100, from 11846 to 11754, meaning that the addition of the "straight" variable to our model did indeed improve its predictive power.
